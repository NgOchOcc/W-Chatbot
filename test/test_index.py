import json
import numpy as np
import pandas as pd
from typing import List, Dict, Any
from pymilvus import Collection, connections
from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import time

class MilvusTokenAnalyzer:
    def __init__(self, host="localhost", port="19530", collection_name="v768_cosine_2", 
                 model_name="Qwen/Qwen2.5-7B-Instruct"):
        """
        Kh·ªüi t·∫°o analyzer
        
        Args:
            host (str): Milvus host
            port (str): Milvus port  
            collection_name (str): T√™n collection
            model_name (str): Model tokenizer ƒë·ªÉ s·ª≠ d·ª•ng
        """
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.model_name = model_name
        
        # K·∫øt n·ªëi Milvus
        self.connect_milvus()
        
        # Kh·ªüi t·∫°o tokenizer
        try:
            print(f"ƒêang t·∫£i tokenizer {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.use_real_tokenizer = True
            print("‚úì Tokenizer ƒë√£ s·∫µn s√†ng!")
        except Exception as e:
            print(f"‚ö† Kh√¥ng th·ªÉ t·∫£i tokenizer th·ª±c: {e}")
            print("S·ª≠ d·ª•ng ∆∞·ªõc t√≠nh heuristic thay th·∫ø...")
            self.tokenizer = None
            self.use_real_tokenizer = False
    
    def connect_milvus(self):
        """K·∫øt n·ªëi t·ªõi Milvus"""
        try:
            connections.connect("default", host=self.host, port=self.port)
            self.collection = Collection(name=self.collection_name)
            self.collection.load()
            print(f"‚úì ƒê√£ k·∫øt n·ªëi Milvus: {self.collection_name}")
            print(f"‚úì Schema: {self.collection.schema}")
            print(f"‚úì T·ªïng s·ªë entities: {self.collection.num_entities}")
        except Exception as e:
            print(f"‚úó L·ªói k·∫øt n·ªëi Milvus: {e}")
            raise
    
    def get_all_texts(self, batch_size=100) -> List[Dict]:
        """
        L·∫•y t·∫•t c·∫£ text t·ª´ collection
        
        Args:
            batch_size (int): K√≠ch th∆∞·ªõc batch ƒë·ªÉ tr√°nh memory overflow
            
        Returns:
            List[Dict]: Danh s√°ch documents v·ªõi id, doc_id, text
        """
        print("ƒêang l·∫•y d·ªØ li·ªáu t·ª´ Milvus...")
        all_docs = []
        offset = 0
        
        while True:
            try:
                # Query v·ªõi limit v√† offset
                results = self.collection.query(
                    expr="",  # L·∫•y t·∫•t c·∫£
                    output_fields=["id", "doc_id", "text"],
                    limit=batch_size,
                    offset=offset
                )
                
                if not results:
                    break
                    
                all_docs.extend(results)
                offset += batch_size
                print(f"‚úì ƒê√£ l·∫•y {len(all_docs)} documents...")
                
                # Tr√°nh v∆∞·ª£t qu√° t·ªïng s·ªë entities
                if len(results) < batch_size:
                    break
                    
            except Exception as e:
                print(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·∫°i offset {offset}: {e}")
                break
        
        print(f"‚úì Ho√†n th√†nh! T·ªïng c·ªông: {len(all_docs)} documents")
        return all_docs
    
    def count_tokens_real(self, text: str) -> int:
        """ƒê·∫øm token s·ª≠ d·ª•ng tokenizer th·∫≠t"""
        try:
            tokens = self.tokenizer.encode(text, add_special_tokens=True)
            return len(tokens)
        except Exception as e:
            print(f"L·ªói tokenization: {e}")
            return self.count_tokens_heuristic(text)
    
    def count_tokens_heuristic(self, text: str) -> int:
        """∆Ø·ªõc t√≠nh token count cho ti·∫øng Romanian"""
        if not text or not text.strip():
            return 0
            
        words = text.split()
        base_tokens = len(words)
        
        # ƒêi·ªÅu ch·ªânh cho ƒë·∫∑c tr∆∞ng Romanian
        romanian_chars = "ƒÉ√¢√Æ»ô»õƒÇ√Ç√é»ò»ö"
        special_char_bonus = sum(1 for c in text if c in romanian_chars) * 0.15
        
        # T·ª´ d√†i
        long_word_bonus = sum(0.4 for word in words if len(word.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) > 8)
        very_long_word_bonus = sum(0.3 for word in words if len(word.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) > 12)
        
        # D·∫•u c√¢u
        punctuation_bonus = sum(1 for c in text if c in ".,;:!?()[]{}\"'`-‚Äì‚Äî") * 0.1
        
        # Complexity cho vƒÉn b·∫£n d√†i
        complexity_bonus = len(words) * 0.05 if len(words) > 50 else 0
        
        estimated = base_tokens + special_char_bonus + long_word_bonus + very_long_word_bonus + punctuation_bonus + complexity_bonus
        return max(1, int(estimated))  # √çt nh·∫•t 1 token
    
    def analyze_single_document(self, doc: Dict) -> Dict:
        """
        Ph√¢n t√≠ch 1 document
        
        Args:
            doc (Dict): Document v·ªõi fields: id, doc_id, text
            
        Returns:
            Dict: Th√¥ng tin ph√¢n t√≠ch chi ti·∫øt
        """
        text = doc.get('text', '')
        
        # ƒê·∫øm token
        if self.use_real_tokenizer:
            token_count = self.count_tokens_real(text)
        else:
            token_count = self.count_tokens_heuristic(text)
        
        # Ph√¢n t√≠ch c∆° b·∫£n
        words = text.split()
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        # K√Ω t·ª± ƒë·∫∑c bi·ªát Romanian
        romanian_chars = "ƒÉ√¢√Æ»ô»õƒÇ√Ç√é»ò»ö"
        romanian_char_count = sum(1 for c in text if c in romanian_chars)
        
        return {
            'id': doc.get('id', ''),
            'doc_id': doc.get('doc_id', ''),
            'text_preview': text[:100] + ('...' if len(text) > 100 else ''),
            'character_count': len(text),
            'word_count': len(words),
            'sentence_count': len(sentences),
            'token_count': token_count,
            'tokens_per_word': token_count / len(words) if words else 0,
            'tokens_per_char': token_count / len(text) if text else 0,
            'avg_word_length': sum(len(w.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) for w in words) / len(words) if words else 0,
            'avg_sentence_length': len(words) / len(sentences) if sentences else 0,
            'romanian_char_count': romanian_char_count,
            'romanian_char_ratio': romanian_char_count / len(text) if text else 0,
            'text_length': len(text)
        }
    
    def analyze_all_documents(self) -> pd.DataFrame:
        """
        Ph√¢n t√≠ch t·∫•t c·∫£ documents
        
        Returns:
            pd.DataFrame: K·∫øt qu·∫£ ph√¢n t√≠ch
        """
        print("\n=== B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH TOKEN COUNT ===")
        
        # L·∫•y d·ªØ li·ªáu
        docs = self.get_all_texts()
        
        if not docs:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch!")
            return pd.DataFrame()
        
        # Ph√¢n t√≠ch t·ª´ng document
        print(f"\nƒêang ph√¢n t√≠ch {len(docs)} documents...")
        results = []
        
        start_time = time.time()
        for i, doc in enumerate(docs):
            if i % 50 == 0:
                elapsed = time.time() - start_time
                print(f"‚úì ƒê√£ x·ª≠ l√Ω {i}/{len(docs)} documents ({elapsed:.1f}s)")
            
            analysis = self.analyze_single_document(doc)
            results.append(analysis)
        
        print(f"‚úì Ho√†n th√†nh ph√¢n t√≠ch! ({time.time() - start_time:.1f}s)")
        
        # T·∫°o DataFrame
        df = pd.DataFrame(results)
        return df
    
    def calculate_statistics(self, df: pd.DataFrame) -> Dict:
        """T√≠nh to√°n th·ªëng k√™ t·ªïng h·ª£p"""
        if df.empty:
            return {}
        
        token_stats = {
            'total_documents': len(df),
            'total_tokens': df['token_count'].sum(),
            'total_words': df['word_count'].sum(), 
            'total_characters': df['character_count'].sum(),
            
            # Token statistics
            'token_min': df['token_count'].min(),
            'token_max': df['token_count'].max(),
            'token_mean': df['token_count'].mean(),
            'token_median': df['token_count'].median(),
            'token_std': df['token_count'].std(),
            'token_q25': df['token_count'].quantile(0.25),
            'token_q75': df['token_count'].quantile(0.75),
            'token_q90': df['token_count'].quantile(0.90),
            'token_q95': df['token_count'].quantile(0.95),
            'token_q99': df['token_count'].quantile(0.99),
            
            # Ratios
            'avg_tokens_per_word': df['tokens_per_word'].mean(),
            'avg_tokens_per_char': df['tokens_per_char'].mean(),
            'avg_word_length': df['avg_word_length'].mean(),
            'avg_sentence_length': df['avg_sentence_length'].mean(),
            
            # Romanian specific
            'avg_romanian_char_ratio': df['romanian_char_ratio'].mean(),
            
            # Distribution
            'short_docs': len(df[df['token_count'] < 100]),
            'medium_docs': len(df[(df['token_count'] >= 100) & (df['token_count'] < 500)]),
            'long_docs': len(df[df['token_count'] >= 500]),
        }
        
        return token_stats
    
    def print_statistics(self, stats: Dict):
        """In th·ªëng k√™ ra m√†n h√¨nh"""
        print("\n" + "="*60)
        print("           TH·ªêNG K√ä TOKEN COUNT T·ªîNG H·ª¢P")
        print("="*60)
        
        print(f"\nüìä T·ªîNG QUAN:")
        print(f"   ‚Ä¢ T·ªïng documents: {stats['total_documents']:,}")
        print(f"   ‚Ä¢ T·ªïng tokens: {stats['total_tokens']:,}")
        print(f"   ‚Ä¢ T·ªïng words: {stats['total_words']:,}")
        print(f"   ‚Ä¢ T·ªïng characters: {stats['total_characters']:,}")
        
        print(f"\nüéØ TH·ªêNG K√ä TOKEN:")
        print(f"   ‚Ä¢ Min: {stats['token_min']:,} tokens")
        print(f"   ‚Ä¢ Max: {stats['token_max']:,} tokens") 
        print(f"   ‚Ä¢ Mean: {stats['token_mean']:,.1f} tokens")
        print(f"   ‚Ä¢ Median: {stats['token_median']:,.1f} tokens")
        print(f"   ‚Ä¢ Std Dev: {stats['token_std']:,.1f}")
        print(f"   ‚Ä¢ Q25: {stats['token_q25']:,.1f}")
        print(f"   ‚Ä¢ Q75: {stats['token_q75']:,.1f}")
        print(f"   ‚Ä¢ Q90: {stats['token_q90']:,.1f}")
        print(f"   ‚Ä¢ Q95: {stats['token_q95']:,.1f}")
        print(f"   ‚Ä¢ Q99: {stats['token_q99']:,.1f}")
        
        print(f"\nüìà T·ª∂ L·ªÜ TRUNG B√åNH:")
        print(f"   ‚Ä¢ Tokens/Word: {stats['avg_tokens_per_word']:.3f}")
        print(f"   ‚Ä¢ Tokens/Char: {stats['avg_tokens_per_char']:.4f}")
        print(f"   ‚Ä¢ ƒê·ªô d√†i TB t·ª´: {stats['avg_word_length']:.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ ƒê·ªô d√†i TB c√¢u: {stats['avg_sentence_length']:.1f} t·ª´")
        print(f"   ‚Ä¢ T·ª∑ l·ªá k√Ω t·ª± Romanian: {stats['avg_romanian_char_ratio']:.3f}")
        
        print(f"\nüìã PH√ÇN LO·∫†I THEO K√çCH TH∆Ø·ªöC:")
        print(f"   ‚Ä¢ Ng·∫Øn (<100 tokens): {stats['short_docs']:,} docs ({stats['short_docs']/stats['total_documents']*100:.1f}%)")
        print(f"   ‚Ä¢ Trung b√¨nh (100-499): {stats['medium_docs']:,} docs ({stats['medium_docs']/stats['total_documents']*100:.1f}%)")
        print(f"   ‚Ä¢ D√†i (‚â•500 tokens): {stats['long_docs']:,} docs ({stats['long_docs']/stats['total_documents']*100:.1f}%)")
    
    def create_token_distribution_buckets(self, df: pd.DataFrame, bucket_size=100) -> Dict:
        """
        T·∫°o ph√¢n b·ªë theo bucket c·ªßa token count
        
        Args:
            df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu
            bucket_size (int): K√≠ch th∆∞·ªõc bucket (m·∫∑c ƒë·ªãnh 100)
            
        Returns:
            Dict: Th√¥ng tin v·ªÅ ph√¢n b·ªë bucket
        """
        if df.empty:
            return {}
        
        max_tokens = df['token_count'].max()
        
        # T·∫°o c√°c bucket ranges
        bucket_ranges = []
        bucket_labels = []
        bucket_counts = []
        
        current = 0
        while current < max_tokens:
            next_val = current + bucket_size
            
            # ƒê·∫øm documents trong bucket n√†y
            count = len(df[(df['token_count'] >= current) & (df['token_count'] < next_val)])
            
            if count > 0:  # Ch·ªâ th√™m bucket c√≥ documents
                bucket_ranges.append((current, next_val))
                bucket_labels.append(f"{current}-{next_val-1}")
                bucket_counts.append(count)
            
            current = next_val
        
        # T·∫°o ph√¢n b·ªë chi ti·∫øt
        distribution = {}
        for i, (start, end) in enumerate(bucket_ranges):
            bucket_data = df[(df['token_count'] >= start) & (df['token_count'] < end)]
            distribution[bucket_labels[i]] = {
                'range': (start, end-1),
                'count': bucket_counts[i],
                'percentage': bucket_counts[i] / len(df) * 100,
                'avg_tokens': bucket_data['token_count'].mean() if len(bucket_data) > 0 else 0,
                'avg_words': bucket_data['word_count'].mean() if len(bucket_data) > 0 else 0,
                'sample_doc_ids': bucket_data['doc_id'].head(3).tolist() if len(bucket_data) > 0 else []
            }
        
        return {
            'bucket_size': bucket_size,
            'total_buckets': len(bucket_ranges),
            'distribution': distribution,
            'bucket_labels': bucket_labels,
            'bucket_counts': bucket_counts,
            'bucket_ranges': bucket_ranges
        }
    
    def plot_token_distribution(self, df: pd.DataFrame, bucket_info: Dict, save_plot=True):
        """
        V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë token count
        
        Args:
            df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu
            bucket_info (Dict): Th√¥ng tin bucket t·ª´ create_token_distribution_buckets
            save_plot (bool): C√≥ l∆∞u bi·ªÉu ƒë·ªì kh√¥ng
        """
        try:
            # Set style
            plt.style.use('default')
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('Token Count Distribution Analysis', fontsize=16, fontweight='bold')
            
            # 1. Histogram v·ªõi buckets
            ax1.bar(bucket_info['bucket_labels'], bucket_info['bucket_counts'], 
                   color='skyblue', alpha=0.7, edgecolor='navy')
            ax1.set_title('Token Distribution by Buckets', fontweight='bold')
            ax1.set_xlabel('Token Range')
            ax1.set_ylabel('Number of Documents')
            ax1.tick_params(axis='x', rotation=45)
            
            # Th√™m annotation cho c√°c bucket c√≥ nhi·ªÅu documents
            for i, (label, count) in enumerate(zip(bucket_info['bucket_labels'], bucket_info['bucket_counts'])):
                if count > len(df) * 0.05:  # Hi·ªÉn th·ªã label n·∫øu > 5% total
                    ax1.text(i, count + 0.5, str(count), ha='center', va='bottom')
            
            # 2. Cumulative distribution
            sorted_tokens = np.sort(df['token_count'])
            cumulative_pct = np.arange(1, len(sorted_tokens) + 1) / len(sorted_tokens) * 100
            ax2.plot(sorted_tokens, cumulative_pct, color='red', linewidth=2)
            ax2.set_title('Cumulative Distribution', fontweight='bold')
            ax2.set_xlabel('Token Count')
            ax2.set_ylabel('Cumulative Percentage')
            ax2.grid(True, alpha=0.3)
            
            # Th√™m quantile lines
            quantiles = [0.5, 0.9, 0.95, 0.99]
            q_values = [df['token_count'].quantile(q) for q in quantiles]
            for q, val in zip(quantiles, q_values):
                ax2.axvline(val, color='orange', linestyle='--', alpha=0.7)
                ax2.text(val, q*100, f'Q{int(q*100)}: {val:.0f}', rotation=90, 
                        verticalalignment='bottom', fontsize=9)
            
            # 3. Box plot
            ax3.boxplot(df['token_count'], vert=True, patch_artist=True,
                       boxprops=dict(facecolor='lightgreen', alpha=0.7))
            ax3.set_title('Token Count Box Plot', fontweight='bold')
            ax3.set_ylabel('Token Count')
            ax3.grid(True, alpha=0.3)
            
            # 4. Pie chart cho size categories
            sizes = [
                len(df[df['token_count'] < 100]),
                len(df[(df['token_count'] >= 100) & (df['token_count'] < 200)]),
                len(df[(df['token_count'] >= 200) & (df['token_count'] < 500)]),
                len(df[df['token_count'] >= 500])
            ]
            labels = ['<100 tokens', '100-199 tokens', '200-499 tokens', '‚â•500 tokens']
            colors = ['lightcoral', 'lightskyblue', 'lightgreen', 'plum']
            
            # Ch·ªâ hi·ªÉn th·ªã categories c√≥ documents
            non_zero_sizes = [(size, label, color) for size, label, color in zip(sizes, labels, colors) if size > 0]
            if non_zero_sizes:
                sizes, labels, colors = zip(*non_zero_sizes)
                wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
                ax4.set_title('Distribution by Size Categories', fontweight='bold')
            
            plt.tight_layout()
            
            if save_plot:
                plt.savefig('token_distribution.png', dpi=300, bbox_inches='tight')
                print(f"üìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: token_distribution.png")
            
            plt.show()
            
        except Exception as e:
            print(f"‚ö† L·ªói khi v·∫Ω bi·ªÉu ƒë·ªì: {e}")
    
    def print_bucket_analysis(self, bucket_info: Dict):
        """In ph√¢n t√≠ch chi ti·∫øt theo bucket"""
        print(f"\nüìä PH√ÇN B·ªê THEO BUCKET ({bucket_info['bucket_size']} tokens/bucket):")
        print("-" * 80)
        
        total_docs = sum(bucket_info['bucket_counts'])
        
        for label, data in bucket_info['distribution'].items():
            print(f"üîπ {label} tokens:")
            print(f"   ‚Ä¢ S·ªë documents: {data['count']} ({data['percentage']:.1f}%)")
            print(f"   ‚Ä¢ TB tokens: {data['avg_tokens']:.1f}")
            print(f"   ‚Ä¢ TB words: {data['avg_words']:.1f}")
            if data['sample_doc_ids']:
                print(f"   ‚Ä¢ M·∫´u doc_ids: {', '.join(data['sample_doc_ids'])}")
        
        print(f"\nüìà TOP 5 BUCKET NHI·ªÄU DOCUMENTS NH·∫§T:")
        sorted_buckets = sorted(bucket_info['distribution'].items(), 
                              key=lambda x: x[1]['count'], reverse=True)[:5]
        
        for i, (label, data) in enumerate(sorted_buckets, 1):
            print(f"   {i}. {label}: {data['count']} docs ({data['percentage']:.1f}%)")

        """T√¨m documents c√≥ token count c·ª±c tr·ªã"""
        print(f"\nüîù TOP {n} DOCUMENTS D√ÄI NH·∫§T:")
        longest = df.nlargest(n, 'token_count')[['doc_id', 'token_count', 'word_count', 'character_count', 'text_preview']]
        for i, row in longest.iterrows():
            print(f"   {row['doc_id']}: {row['token_count']} tokens, {row['word_count']} words")
            print(f"      Preview: {row['text_preview']}")
        
        print(f"\nüîª TOP {n} DOCUMENTS NG·∫ÆN NH·∫§T:")
        shortest = df.nsmallest(n, 'token_count')[['doc_id', 'token_count', 'word_count', 'character_count', 'text_preview']]
        for i, row in shortest.iterrows():
            print(f"   {row['doc_id']}: {row['token_count']} tokens, {row['word_count']} words")
            print(f"      Preview: {row['text_preview']}")
    
    def save_results(self, df: pd.DataFrame, stats: Dict, bucket_info: Dict, filename_prefix="milvus_token_analysis"):
        """L∆∞u k·∫øt qu·∫£ ra file"""
        try:
            # L∆∞u DataFrame
            csv_file = f"{filename_prefix}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"\nüíæ ƒê√£ l∆∞u chi ti·∫øt: {csv_file}")
            
            # L∆∞u th·ªëng k√™ t·ªïng h·ª£p bao g·ªìm bucket info
            stats_file = f"{filename_prefix}_stats.json"
            combined_stats = {**stats, 'bucket_analysis': bucket_info}
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                # Convert numpy types to native Python types for JSON serialization
                json_stats = {}
                for k, v in combined_stats.items():
                    if isinstance(v, (np.integer, np.floating)):
                        json_stats[k] = v.item()
                    elif isinstance(v, dict):
                        json_stats[k] = {}
                        for k2, v2 in v.items():
                            if isinstance(v2, (np.integer, np.floating)):
                                json_stats[k][k2] = v2.item()
                            else:
                                json_stats[k][k2] = v2
                    else:
                        json_stats[k] = v
                        
                json.dump(json_stats, f, ensure_ascii=False, indent=2)
            print(f"üíæ ƒê√£ l∆∞u th·ªëng k√™: {stats_file}")
            
            # L∆∞u bucket distribution ri√™ng
            bucket_file = f"{filename_prefix}_buckets.csv"
            bucket_df = pd.DataFrame([
                {
                    'bucket_range': label,
                    'start_token': data['range'][0],
                    'end_token': data['range'][1], 
                    'doc_count': data['count'],
                    'percentage': data['percentage'],
                    'avg_tokens': data['avg_tokens'],
                    'avg_words': data['avg_words'],
                    'sample_docs': '; '.join(data['sample_doc_ids'])
                }
                for label, data in bucket_info['distribution'].items()
            ])
            bucket_df.to_csv(bucket_file, index=False, encoding='utf-8')
            print(f"üíæ ƒê√£ l∆∞u bucket analysis: {bucket_file}")
            
        except Exception as e:
            print(f"‚ö† L·ªói khi l∆∞u file: {e}")
    
    def run_full_analysis(self, save_results=True, bucket_size=100, create_plots=True):
        """Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"""
        print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH ƒê·∫¶Y ƒê·ª¶ MILVUS COLLECTION")
        print(f"üìç Collection: {self.collection_name}")
        print(f"ü§ñ Tokenizer: {'Real ' + self.model_name if self.use_real_tokenizer else 'Heuristic estimation'}")
        print(f"üìä Bucket size: {bucket_size} tokens")
        
        # Ph√¢n t√≠ch
        df = self.analyze_all_documents()
        
        if df.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch!")
            return None, None, None
        
        # T√≠nh th·ªëng k√™ c∆° b·∫£n
        stats = self.calculate_statistics(df)
        
        # T·∫°o bucket analysis
        bucket_info = self.create_token_distribution_buckets(df, bucket_size)
        
        # In k·∫øt qu·∫£
        self.print_statistics(stats)
        self.print_bucket_analysis(bucket_info)
        self.find_extremes(df)
        
        # T·∫°o bi·ªÉu ƒë·ªì
        if create_plots:
            self.plot_token_distribution(df, bucket_info, save_plot=save_results)
        
        # L∆∞u k·∫øt qu·∫£
        if save_results:
            self.save_results(df, stats, bucket_info)
        
        print("\n‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH!")
        return df, stats, bucket_info

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    # Kh·ªüi t·∫°o analyzer
    analyzer = MilvusTokenAnalyzer(
        host="localhost", 
        port="19530",
        collection_name="v768_cosine_2",
        model_name="Qwen/Qwen2.5-7B-Instruct"
    )
    
    # Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß v·ªõi bucket size 100 tokens
    df, stats, bucket_info = analyzer.run_full_analysis(
        save_results=True, 
        bucket_size=100,  # C√≥ th·ªÉ thay ƒë·ªïi: 50, 100, 200...
        create_plots=True
    )
    
    # C√≥ th·ªÉ s·ª≠ d·ª•ng k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch th√™m
    if df is not None and stats is not None:
        print(f"\nüîç M·ªòT S·ªê INSIGHT TH√äM:")
        print(f"   ‚Ä¢ Document c√≥ nhi·ªÅu token nh·∫•t: {df.loc[df['token_count'].idxmax(), 'doc_id']}")
        print(f"   ‚Ä¢ Document c√≥ √≠t token nh·∫•t: {df.loc[df['token_count'].idxmin(), 'doc_id']}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá token/word cao nh·∫•t: {df['tokens_per_word'].max():.3f}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá token/word th·∫•p nh·∫•t: {df['tokens_per_word'].min():.3f}")
        
        # Context window analysis
        contexts = [1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]
        print(f"\nüìè PH√ÇN T√çCH CONTEXT WINDOW:")
        for ctx in contexts:
            fitting_docs = len(df[df['token_count'] <= ctx])
            percentage = fitting_docs / len(df) * 100
            print(f"   ‚Ä¢ {ctx:,} tokens: {fitting_docs:,}/{len(df):,} docs ({percentage:.1f}%)")
        
        # Bucket insights
        if bucket_info and bucket_info['distribution']:
            print(f"\nü™£ BUCKET INSIGHTS:")
            print(f"   ‚Ä¢ T·ªïng s·ªë buckets c√≥ data: {bucket_info['total_buckets']}")
            
            # T√¨m bucket c√≥ nhi·ªÅu documents nh·∫•t
            max_bucket = max(bucket_info['distribution'].items(), key=lambda x: x[1]['count'])
            print(f"   ‚Ä¢ Bucket ph·ªï bi·∫øn nh·∫•t: {max_bucket[0]} ({max_bucket[1]['count']} docs)")
            
            # T√≠nh ph·∫ßn trƒÉm documents trong c√°c bucket ch√≠nh
            total_docs = sum([data['count'] for data in bucket_info['distribution'].values()])
            under_500 = sum([data['count'] for label, data in bucket_info['distribution'].items() 
                           if data['range'][1] < 500])
            print(f"   ‚Ä¢ Documents < 500 tokens: {under_500}/{total_docs} ({under_500/total_docs*100:.1f}%)")
            
    print(f"\nüìÅ FILES GENERATED:")
    print(f"   ‚Ä¢ milvus_token_analysis.csv - Chi ti·∫øt t·ª´ng document")
    print(f"   ‚Ä¢ milvus_token_analysis_stats.json - Th·ªëng k√™ t·ªïng h·ª£p")  
    print(f"   ‚Ä¢ milvus_token_analysis_buckets.csv - Ph√¢n b·ªë bucket")
    print(f"   ‚Ä¢ token_distribution.png - Bi·ªÉu ƒë·ªì ph√¢n b·ªë")

