import json
import time
from typing import List, Dict

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pymilvus import Collection, connections
from transformers import AutoTokenizer


class MilvusTokenAnalyzer:
    def __init__(self, host="localhost", port="19530", collection_name="v768_cosine_2",
                 model_name="Qwen/Qwen2.5-7B-Instruct"):
        """
        Kh·ªüi t·∫°o analyzer

        Args:
            host (str): Milvus host
            port (str): Milvus port
            collection_name (str): T√™n collection
            model_name (str): Model tokenizer ƒë·ªÉ s·ª≠ d·ª•ng
        """
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.model_name = model_name

        # K·∫øt n·ªëi Milvus
        self.connect_milvus()

        # Kh·ªüi t·∫°o tokenizer
        try:
            print(f"ƒêang t·∫£i tokenizer {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.use_real_tokenizer = True
            print("‚úì Tokenizer ƒë√£ s·∫µn s√†ng!")
        except Exception as e:
            print(f"‚ö† Kh√¥ng th·ªÉ t·∫£i tokenizer th·ª±c: {e}")
            print("S·ª≠ d·ª•ng ∆∞·ªõc t√≠nh heuristic thay th·∫ø...")
            self.tokenizer = None
            self.use_real_tokenizer = False

    def connect_milvus(self):
        """K·∫øt n·ªëi t·ªõi Milvus"""
        try:
            connections.connect("default", host=self.host, port=self.port)
            self.collection = Collection(name=self.collection_name)
            self.collection.load()
            print(f"‚úì ƒê√£ k·∫øt n·ªëi Milvus: {self.collection_name}")
            print(f"‚úì Schema: {self.collection.schema}")
            print(f"‚úì T·ªïng s·ªë entities: {self.collection.num_entities}")
        except Exception as e:
            print(f"‚úó L·ªói k·∫øt n·ªëi Milvus: {e}")
            raise

    def get_all_texts(self, batch_size=100) -> List[Dict]:
        print("ƒêang l·∫•y d·ªØ li·ªáu t·ª´ Milvus...")
        all_docs = []
        offset = 0

        while True:
            try:
                # Query v·ªõi limit v√† offset
                results = self.collection.query(
                    expr="",  # L·∫•y t·∫•t c·∫£
                    output_fields=["id", "doc_id", "text"],
                    limit=batch_size,
                    offset=offset
                )

                if not results:
                    break

                all_docs.extend(results)
                offset += batch_size
                print(f"‚úì ƒê√£ l·∫•y {len(all_docs)} documents...")

                # Tr√°nh v∆∞·ª£t qu√° t·ªïng s·ªë entities
                if len(results) < batch_size:
                    break

            except Exception as e:
                print(f"L·ªói khi l·∫•y d·ªØ li·ªáu t·∫°i offset {offset}: {e}")
                break

        print(f"‚úì Ho√†n th√†nh! T·ªïng c·ªông: {len(all_docs)} documents")
        return all_docs

    def count_tokens_real(self, text: str) -> int:
        """ƒê·∫øm token s·ª≠ d·ª•ng tokenizer th·∫≠t"""
        try:
            tokens = self.tokenizer.encode(text, add_special_tokens=True)
            return len(tokens)
        except Exception as e:
            print(f"L·ªói tokenization: {e}")
            return self.count_tokens_heuristic(text)

    def count_tokens_heuristic(self, text: str) -> int:
        """∆Ø·ªõc t√≠nh token count cho ti·∫øng Romanian"""
        if not text or not text.strip():
            return 0

        words = text.split()
        base_tokens = len(words)

        # ƒêi·ªÅu ch·ªânh cho ƒë·∫∑c tr∆∞ng Romanian
        romanian_chars = "ƒÉ√¢√Æ»ô»õƒÇ√Ç√é»ò»ö"
        special_char_bonus = sum(1 for c in text if c in romanian_chars) * 0.15

        # T·ª´ d√†i
        long_word_bonus = sum(0.4 for word in words if len(word.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) > 8)
        very_long_word_bonus = sum(0.3 for word in words if len(word.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) > 12)

        # D·∫•u c√¢u
        punctuation_bonus = sum(1 for c in text if c in ".,;:!?()[]{}\"'`-‚Äì‚Äî") * 0.1

        # Complexity cho vƒÉn b·∫£n d√†i
        complexity_bonus = len(words) * 0.05 if len(words) > 50 else 0

        estimated = base_tokens + special_char_bonus + long_word_bonus + very_long_word_bonus + punctuation_bonus + complexity_bonus  # noqa
        return max(1, int(estimated))  # √çt nh·∫•t 1 token

    def analyze_single_document(self, doc: Dict) -> Dict:
        text = doc.get('text', '')

        # ƒê·∫øm token
        if self.use_real_tokenizer:
            token_count = self.count_tokens_real(text)
        else:
            token_count = self.count_tokens_heuristic(text)

        # Ph√¢n t√≠ch c∆° b·∫£n
        words = text.split()
        sentences = [s.strip() for s in text.split('.') if s.strip()]

        # K√Ω t·ª± ƒë·∫∑c bi·ªát Romanian
        romanian_chars = "ƒÉ√¢√Æ»ô»õƒÇ√Ç√é»ò»ö"
        romanian_char_count = sum(1 for c in text if c in romanian_chars)

        return {
            'id': doc.get('id', ''),
            'doc_id': doc.get('doc_id', ''),
            'text_preview': text[:100] + ('...' if len(text) > 100 else ''),
            'character_count': len(text),
            'word_count': len(words),
            'sentence_count': len(sentences),
            'token_count': token_count,
            'tokens_per_word': token_count / len(words) if words else 0,
            'tokens_per_char': token_count / len(text) if text else 0,
            'avg_word_length': sum(len(w.strip('.,;:!?()[]{}\"\'`-‚Äì‚Äî')) for w in words) / len(words) if words else 0,
            'avg_sentence_length': len(words) / len(sentences) if sentences else 0,
            'romanian_char_count': romanian_char_count,
            'romanian_char_ratio': romanian_char_count / len(text) if text else 0,
            'text_length': len(text)
        }

    def analyze_all_documents(self) -> pd.DataFrame:
        print("\n=== B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH TOKEN COUNT ===")

        # L·∫•y d·ªØ li·ªáu
        docs = self.get_all_texts()

        if not docs:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch!")
            return pd.DataFrame()

        # Ph√¢n t√≠ch t·ª´ng document
        print(f"\nƒêang ph√¢n t√≠ch {len(docs)} documents...")
        results = []

        start_time = time.time()
        for i, doc in enumerate(docs):
            if i % 50 == 0:
                elapsed = time.time() - start_time
                print(f"‚úì ƒê√£ x·ª≠ l√Ω {i}/{len(docs)} documents ({elapsed:.1f}s)")

            analysis = self.analyze_single_document(doc)
            results.append(analysis)

        print(f"‚úì Ho√†n th√†nh ph√¢n t√≠ch! ({time.time() - start_time:.1f}s)")

        # T·∫°o DataFrame
        df = pd.DataFrame(results)
        return df

    def calculate_statistics(self, df: pd.DataFrame) -> Dict:
        """T√≠nh to√°n th·ªëng k√™ t·ªïng h·ª£p"""
        if df.empty:
            return {}

        token_stats = {
            'total_documents': len(df),
            'total_tokens': df['token_count'].sum(),
            'total_words': df['word_count'].sum(),
            'total_characters': df['character_count'].sum(),

            # Token statistics
            'token_min': df['token_count'].min(),
            'token_max': df['token_count'].max(),
            'token_mean': df['token_count'].mean(),
            'token_median': df['token_count'].median(),
            'token_std': df['token_count'].std(),
            'token_q25': df['token_count'].quantile(0.25),
            'token_q75': df['token_count'].quantile(0.75),
            'token_q90': df['token_count'].quantile(0.90),
            'token_q95': df['token_count'].quantile(0.95),
            'token_q99': df['token_count'].quantile(0.99),

            # Ratios
            'avg_tokens_per_word': df['tokens_per_word'].mean(),
            'avg_tokens_per_char': df['tokens_per_char'].mean(),
            'avg_word_length': df['avg_word_length'].mean(),
            'avg_sentence_length': df['avg_sentence_length'].mean(),

            # Romanian specific
            'avg_romanian_char_ratio': df['romanian_char_ratio'].mean(),

            # Distribution
            'short_docs': len(df[df['token_count'] < 100]),
            'medium_docs': len(df[(df['token_count'] >= 100) & (df['token_count'] < 500)]),
            'long_docs': len(df[df['token_count'] >= 500]),
        }

        return token_stats

    def print_statistics(self, stats: Dict):
        """In th·ªëng k√™ ra m√†n h√¨nh"""
        print("\n" + "=" * 60)
        print("           TH·ªêNG K√ä TOKEN COUNT T·ªîNG H·ª¢P")
        print("=" * 60)

        print("\nüìä T·ªîNG QUAN:")
        print(f"   ‚Ä¢ T·ªïng documents: {stats['total_documents']:,}")
        print(f"   ‚Ä¢ T·ªïng tokens: {stats['total_tokens']:,}")
        print(f"   ‚Ä¢ T·ªïng words: {stats['total_words']:,}")
        print(f"   ‚Ä¢ T·ªïng characters: {stats['total_characters']:,}")

        print("\nüéØ TH·ªêNG K√ä TOKEN:")
        print(f"   ‚Ä¢ Min: {stats['token_min']:,} tokens")
        print(f"   ‚Ä¢ Max: {stats['token_max']:,} tokens")
        print(f"   ‚Ä¢ Mean: {stats['token_mean']:,.1f} tokens")
        print(f"   ‚Ä¢ Median: {stats['token_median']:,.1f} tokens")
        print(f"   ‚Ä¢ Std Dev: {stats['token_std']:,.1f}")
        print(f"   ‚Ä¢ Q25: {stats['token_q25']:,.1f}")
        print(f"   ‚Ä¢ Q75: {stats['token_q75']:,.1f}")
        print(f"   ‚Ä¢ Q90: {stats['token_q90']:,.1f}")
        print(f"   ‚Ä¢ Q95: {stats['token_q95']:,.1f}")
        print(f"   ‚Ä¢ Q99: {stats['token_q99']:,.1f}")

        print("\nüìà T·ª∂ L·ªÜ TRUNG B√åNH:")
        print(f"   ‚Ä¢ Tokens/Word: {stats['avg_tokens_per_word']:.3f}")
        print(f"   ‚Ä¢ Tokens/Char: {stats['avg_tokens_per_char']:.4f}")
        print(f"   ‚Ä¢ ƒê·ªô d√†i TB t·ª´: {stats['avg_word_length']:.1f} k√Ω t·ª±")
        print(f"   ‚Ä¢ ƒê·ªô d√†i TB c√¢u: {stats['avg_sentence_length']:.1f} t·ª´")
        print(f"   ‚Ä¢ T·ª∑ l·ªá k√Ω t·ª± Romanian: {stats['avg_romanian_char_ratio']:.3f}")

        print("\nüìã PH√ÇN LO·∫†I THEO K√çCH TH∆Ø·ªöC:")
        print(
            f"   ‚Ä¢ Ng·∫Øn (<100 tokens): {stats['short_docs']:,} docs ({stats['short_docs'] / stats['total_documents'] * 100:.1f}%)")  # noqa
        print(
            f"   ‚Ä¢ Trung b√¨nh (100-499): {stats['medium_docs']:,} docs ({stats['medium_docs'] / stats['total_documents'] * 100:.1f}%)")  # noqa
        print(
            f"   ‚Ä¢ D√†i (‚â•500 tokens): {stats['long_docs']:,} docs ({stats['long_docs'] / stats['total_documents'] * 100:.1f}%)")  # noqa

    def create_token_distribution_buckets(self, df: pd.DataFrame, bucket_size=100) -> Dict:
        if df.empty:
            return {}

        max_tokens = df['token_count'].max()

        # T·∫°o c√°c bucket ranges
        bucket_ranges = []
        bucket_labels = []
        bucket_counts = []

        current = 0
        while current < max_tokens:
            next_val = current + bucket_size

            # ƒê·∫øm documents trong bucket n√†y
            count = len(df[(df['token_count'] >= current) & (df['token_count'] < next_val)])

            if count > 0:  # Ch·ªâ th√™m bucket c√≥ documents
                bucket_ranges.append((current, next_val))
                bucket_labels.append(f"{current}-{next_val - 1}")
                bucket_counts.append(count)

            current = next_val

        # T·∫°o ph√¢n b·ªë chi ti·∫øt
        distribution = {}
        for i, (start, end) in enumerate(bucket_ranges):
            bucket_data = df[(df['token_count'] >= start) & (df['token_count'] < end)]
            distribution[bucket_labels[i]] = {
                'range': (start, end - 1),
                'count': bucket_counts[i],
                'percentage': bucket_counts[i] / len(df) * 100,
                'avg_tokens': bucket_data['token_count'].mean() if len(bucket_data) > 0 else 0,
                'avg_words': bucket_data['word_count'].mean() if len(bucket_data) > 0 else 0,
                'sample_doc_ids': bucket_data['doc_id'].head(3).tolist() if len(bucket_data) > 0 else []
            }

        return {
            'bucket_size': bucket_size,
            'total_buckets': len(bucket_ranges),
            'distribution': distribution,
            'bucket_labels': bucket_labels,
            'bucket_counts': bucket_counts,
            'bucket_ranges': bucket_ranges
        }

    def plot_token_distribution(self, df: pd.DataFrame, bucket_info: Dict, save_plot=True):
        try:
            # Set style
            plt.style.use('default')
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('Token Count Distribution Analysis', fontsize=16, fontweight='bold')

            # 1. Histogram v·ªõi buckets
            ax1.bar(bucket_info['bucket_labels'], bucket_info['bucket_counts'],
                    color='skyblue', alpha=0.7, edgecolor='navy')
            ax1.set_title('Token Distribution by Buckets', fontweight='bold')
            ax1.set_xlabel('Token Range')
            ax1.set_ylabel('Number of Documents')
            ax1.tick_params(axis='x', rotation=45)

            # Th√™m annotation cho c√°c bucket c√≥ nhi·ªÅu documents
            for i, (label, count) in enumerate(zip(bucket_info['bucket_labels'], bucket_info['bucket_counts'])):
                if count > len(df) * 0.05:  # Hi·ªÉn th·ªã label n·∫øu > 5% total
                    ax1.text(i, count + 0.5, str(count), ha='center', va='bottom')

            # 2. Cumulative distribution
            sorted_tokens = np.sort(df['token_count'])
            cumulative_pct = np.arange(1, len(sorted_tokens) + 1) / len(sorted_tokens) * 100
            ax2.plot(sorted_tokens, cumulative_pct, color='red', linewidth=2)
            ax2.set_title('Cumulative Distribution', fontweight='bold')
            ax2.set_xlabel('Token Count')
            ax2.set_ylabel('Cumulative Percentage')
            ax2.grid(True, alpha=0.3)

            # Th√™m quantile lines
            quantiles = [0.5, 0.9, 0.95, 0.99]
            q_values = [df['token_count'].quantile(q) for q in quantiles]
            for q, val in zip(quantiles, q_values):
                ax2.axvline(val, color='orange', linestyle='--', alpha=0.7)
                ax2.text(val, q * 100, f'Q{int(q * 100)}: {val:.0f}', rotation=90,
                         verticalalignment='bottom', fontsize=9)

            # 3. Box plot
            ax3.boxplot(df['token_count'], vert=True, patch_artist=True,
                        boxprops=dict(facecolor='lightgreen', alpha=0.7))
            ax3.set_title('Token Count Box Plot', fontweight='bold')
            ax3.set_ylabel('Token Count')
            ax3.grid(True, alpha=0.3)

            # 4. Pie chart cho size categories
            sizes = [
                len(df[df['token_count'] < 100]),
                len(df[(df['token_count'] >= 100) & (df['token_count'] < 200)]),
                len(df[(df['token_count'] >= 200) & (df['token_count'] < 500)]),
                len(df[df['token_count'] >= 500])
            ]
            labels = ['<100 tokens', '100-199 tokens', '200-499 tokens', '‚â•500 tokens']
            colors = ['lightcoral', 'lightskyblue', 'lightgreen', 'plum']

            # Ch·ªâ hi·ªÉn th·ªã categories c√≥ documents
            non_zero_sizes = [(size, label, color) for size, label, color in zip(sizes, labels, colors) if size > 0]
            if non_zero_sizes:
                sizes, labels, colors = zip(*non_zero_sizes)
                wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                                   startangle=90)
                ax4.set_title('Distribution by Size Categories', fontweight='bold')

            plt.tight_layout()

            if save_plot:
                plt.savefig('token_distribution.png', dpi=300, bbox_inches='tight')
                print("üìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: token_distribution.png")

            plt.show()

        except Exception as e:
            print(f"‚ö† L·ªói khi v·∫Ω bi·ªÉu ƒë·ªì: {e}")

    # def print_bucket_analysis(self, bucket_info: Dict):
    #     """In ph√¢n t√≠ch chi ti·∫øt theo bucket"""
    #     print(f"\nüìä PH√ÇN B·ªê THEO BUCKET ({bucket_info['bucket_size']} tokens/bucket):")
    #     print("-" * 80)
    #
    #     total_docs = sum(bucket_info['bucket_counts'])
    #
    #     for label, data in bucket_info['distribution'].items():
    #         print(f"üîπ {label} tokens:")
    #         print(f"   ‚Ä¢ S·ªë documents: {data['count']} ({data['percentage']:.1f}%)")
    #         print(f"   ‚Ä¢ TB tokens: {data['avg_tokens']:.1f}")
    #         print(f"   ‚Ä¢ TB words: {data['avg_words']:.1f}")
    #         if data['sample_doc_ids']:
    #             print(f"   ‚Ä¢ M·∫´u doc_ids: {', '.join(data['sample_doc_ids'])}")
    #
    #     print(f"\nüìà TOP 5 BUCKET NHI·ªÄU DOCUMENTS NH·∫§T:")
    #     sorted_buckets = sorted(bucket_info['distribution'].items(),
    #                             key=lambda x: x[1]['count'], reverse=True)[:5]
    #
    #     for i, (label, data) in enumerate(sorted_buckets, 1):
    #         print(f"   {i}. {label}: {data['count']} docs ({data['percentage']:.1f}%)")
    #
    #     """T√¨m documents c√≥ token count c·ª±c tr·ªã"""
    #     print(f"\nüîù TOP {n} DOCUMENTS D√ÄI NH·∫§T:")
    #     longest = df.nlargest(n, 'token_count')[
    #         ['doc_id', 'token_count', 'word_count', 'character_count', 'text_preview']]
    #     for i, row in longest.iterrows():
    #         print(f"   {row['doc_id']}: {row['token_count']} tokens, {row['word_count']} words")
    #         print(f"      Preview: {row['text_preview']}")
    #
    #     print(f"\nüîª TOP {n} DOCUMENTS NG·∫ÆN NH·∫§T:")
    #     shortest = df.nsmallest(n, 'token_count')[
    #         ['doc_id', 'token_count', 'word_count', 'character_count', 'text_preview']]
    #     for i, row in shortest.iterrows():
    #         print(f"   {row['doc_id']}: {row['token_count']} tokens, {row['word_count']} words")
    #         print(f"      Preview: {row['text_preview']}")

    def save_results(self, df: pd.DataFrame, stats: Dict, bucket_info: Dict, filename_prefix="milvus_token_analysis"):
        """L∆∞u k·∫øt qu·∫£ ra file"""
        try:
            # L∆∞u DataFrame
            csv_file = f"{filename_prefix}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"\nüíæ ƒê√£ l∆∞u chi ti·∫øt: {csv_file}")

            # L∆∞u th·ªëng k√™ t·ªïng h·ª£p bao g·ªìm bucket info
            stats_file = f"{filename_prefix}_stats.json"
            combined_stats = {**stats, 'bucket_analysis': bucket_info}

            with open(stats_file, 'w', encoding='utf-8') as f:
                # Convert numpy types to native Python types for JSON serialization
                json_stats = {}
                for k, v in combined_stats.items():
                    if isinstance(v, (np.integer, np.floating)):
                        json_stats[k] = v.item()
                    elif isinstance(v, dict):
                        json_stats[k] = {}
                        for k2, v2 in v.items():
                            if isinstance(v2, (np.integer, np.floating)):
                                json_stats[k][k2] = v2.item()
                            else:
                                json_stats[k][k2] = v2
                    else:
                        json_stats[k] = v

                json.dump(json_stats, f, ensure_ascii=False, indent=2)
            print(f"üíæ ƒê√£ l∆∞u th·ªëng k√™: {stats_file}")

            # L∆∞u bucket distribution ri√™ng
            bucket_file = f"{filename_prefix}_buckets.csv"
            bucket_df = pd.DataFrame([
                {
                    'bucket_range': label,
                    'start_token': data['range'][0],
                    'end_token': data['range'][1],
                    'doc_count': data['count'],
                    'percentage': data['percentage'],
                    'avg_tokens': data['avg_tokens'],
                    'avg_words': data['avg_words'],
                    'sample_docs': '; '.join(data['sample_doc_ids'])
                }
                for label, data in bucket_info['distribution'].items()
            ])
            bucket_df.to_csv(bucket_file, index=False, encoding='utf-8')
            print(f"üíæ ƒê√£ l∆∞u bucket analysis: {bucket_file}")

        except Exception as e:
            print(f"‚ö† L·ªói khi l∆∞u file: {e}")

    def run_full_analysis(self, save_results=True, bucket_size=100, create_plots=True):
        """Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"""
        print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH ƒê·∫¶Y ƒê·ª¶ MILVUS COLLECTION")
        print(f"üìç Collection: {self.collection_name}")
        print(f"ü§ñ Tokenizer: {'Real ' + self.model_name if self.use_real_tokenizer else 'Heuristic estimation'}")
        print(f"üìä Bucket size: {bucket_size} tokens")

        # Ph√¢n t√≠ch
        df = self.analyze_all_documents()

        if df.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch!")
            return None, None, None

        # T√≠nh th·ªëng k√™ c∆° b·∫£n
        stats = self.calculate_statistics(df)

        # T·∫°o bucket analysis
        bucket_info = self.create_token_distribution_buckets(df, bucket_size)

        # In k·∫øt qu·∫£
        self.print_statistics(stats)
        self.print_bucket_analysis(bucket_info)
        self.find_extremes(df)

        # T·∫°o bi·ªÉu ƒë·ªì
        if create_plots:
            self.plot_token_distribution(df, bucket_info, save_plot=save_results)

        # L∆∞u k·∫øt qu·∫£
        if save_results:
            self.save_results(df, stats, bucket_info)

        print("\n‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH!")
        return df, stats, bucket_info


# S·ª≠ d·ª•ng
if __name__ == "__main__":
    # Kh·ªüi t·∫°o analyzer
    analyzer = MilvusTokenAnalyzer(
        host="localhost",
        port="19530",
        collection_name="v768_cosine_2",
        model_name="Qwen/Qwen2.5-7B-Instruct"
    )

    # Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß v·ªõi bucket size 100 tokens
    df, stats, bucket_info = analyzer.run_full_analysis(
        save_results=True,
        bucket_size=100,  # C√≥ th·ªÉ thay ƒë·ªïi: 50, 100, 200...
        create_plots=True
    )

    # C√≥ th·ªÉ s·ª≠ d·ª•ng k·∫øt qu·∫£ ƒë·ªÉ ph√¢n t√≠ch th√™m
    if df is not None and stats is not None:
        print("\nüîç M·ªòT S·ªê INSIGHT TH√äM:")
        print(f"   ‚Ä¢ Document c√≥ nhi·ªÅu token nh·∫•t: {df.loc[df['token_count'].idxmax(), 'doc_id']}")
        print(f"   ‚Ä¢ Document c√≥ √≠t token nh·∫•t: {df.loc[df['token_count'].idxmin(), 'doc_id']}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá token/word cao nh·∫•t: {df['tokens_per_word'].max():.3f}")
        print(f"   ‚Ä¢ T·ª∑ l·ªá token/word th·∫•p nh·∫•t: {df['tokens_per_word'].min():.3f}")

        # Context window analysis
        contexts = [1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]
        print("\nüìè PH√ÇN T√çCH CONTEXT WINDOW:")
        for ctx in contexts:
            fitting_docs = len(df[df['token_count'] <= ctx])
            percentage = fitting_docs / len(df) * 100
            print(f"   ‚Ä¢ {ctx:,} tokens: {fitting_docs:,}/{len(df):,} docs ({percentage:.1f}%)")

        # Bucket insights
        if bucket_info and bucket_info['distribution']:
            print("\nü™£ BUCKET INSIGHTS:")
            print(f"   ‚Ä¢ T·ªïng s·ªë buckets c√≥ data: {bucket_info['total_buckets']}")

            # T√¨m bucket c√≥ nhi·ªÅu documents nh·∫•t
            max_bucket = max(bucket_info['distribution'].items(), key=lambda x: x[1]['count'])
            print(f"   ‚Ä¢ Bucket ph·ªï bi·∫øn nh·∫•t: {max_bucket[0]} ({max_bucket[1]['count']} docs)")

            # T√≠nh ph·∫ßn trƒÉm documents trong c√°c bucket ch√≠nh
            total_docs = sum([data['count'] for data in bucket_info['distribution'].values()])
            under_500 = sum([data['count'] for label, data in bucket_info['distribution'].items()
                             if data['range'][1] < 500])
            print(f"   ‚Ä¢ Documents < 500 tokens: {under_500}/{total_docs} ({under_500 / total_docs * 100:.1f}%)")

    print("\nüìÅ FILES GENERATED:")
    print("   ‚Ä¢ milvus_token_analysis.csv - Chi ti·∫øt t·ª´ng document")
    print("   ‚Ä¢ milvus_token_analysis_stats.json - Th·ªëng k√™ t·ªïng h·ª£p")
    print("   ‚Ä¢ milvus_token_analysis_buckets.csv - Ph√¢n b·ªë bucket")
    print("   ‚Ä¢ token_distribution.png - Bi·ªÉu ƒë·ªì ph√¢n b·ªë")
